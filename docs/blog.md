# Building an Autonomous SOC on Elastic: Architecture Decisions and Lessons

*A technical blog documenting the design decisions behind the Elastic Security Agent Mesh — a fully autonomous Security Operations Centre built natively on the Elastic platform.*

---

## The Vision

The goal is ambitious: build a self-sustaining SOC that can automatically generate detections, triage alerts, investigate threats, respond to incidents, and improve itself over time — all running natively on Elastic. Not a bolt-on AI layer, but a mesh of specialist agents that use Elastic as both their reasoning plane and their action plane.

The end state is a system where:

- A **Detection Engineering Agent** writes rules based on the threat landscape and data availability, creating detections that map to MITRE ATT&CK and target ECS-normalised fields.
- A **Security Analyst Agent** triages alerts, enriches them with threat intelligence, and escalates when needed.
- A **Threat Intelligence Agent** researches IOCs, tracks campaigns, and feeds context back to the detection engineer.
- A **Forensics Agent** performs deep investigation on endpoints when the analyst finds something worth digging into.
- A **Compliance Agent** maps controls to regulations and identifies gaps.
- A **SOC Operations Agent** coordinates shift schedules, escalations, and operational logistics.

All of them can discover each other, hand off investigations, share evidence, and take actions — with governance controls that prevent them from doing anything destructive without human approval.

---

## Evaluating the Starting Point: What OpenAI Codex Proposed

We began by evaluating a plan generated by OpenAI Codex for this project. The plan proposed a "policy-driven multi-agent mesh" with risk-tiered controls, a 90-day delivery path, and a separation into reasoning, execution, and governance planes.

### What the Codex plan got right

- **Tiered risk model (Tier 0/1/2)** — the idea of categorising agent actions by blast radius is genuinely sound and became a cornerstone of our architecture.
- **Staying Elastic-native** — avoiding a second control plane and keeping Elastic as both the data store and action plane was the right call.
- **Agent-first orchestration** — the shift from workflow-driven to agent-driven routing was directionally correct.
- **OpenClaw as sandbox only** — keeping external frameworks out of production orchestration until the core mesh is mature.

### Where the plan fell short

The Codex plan read like a consultant's slide deck: the right headings, reasonable structure, but it didn't wrestle with the actual hard problems.

**1. State management was completely absent.** This is the hardest problem in a multi-agent architecture. When Agent A triages an alert, enriches it via Agent B, escalates to Agent C, and creates a case — where does the investigation context live? Who owns it? How do agents access each other's findings? The Codex plan didn't mention this at all. Our solution was to design the `investigation-contexts` index as a shared state object with structured evidence chains, pending actions, and semantic summaries.

**2. Governance was hand-waved.** "Encode allowed caller agents, required evidence/confidence thresholds, blast-radius limits" — yes, but *how*, inside Elastic's workflow engine? Workflows are deterministic YAML pipelines. You can't just "add a risk gate" without designing the concrete mechanism. We designed a policy index (`action-policies`) with a lookup workflow that agents call before every action, returning the tier and approval requirements.

**3. No feedback loops.** The entire value proposition of an autonomous SOC is that it learns and improves. The Codex plan mentioned "evaluation loops" in one bullet point but didn't design them. How does the Detection Engineering Agent know its rules are producing false positives? How does triage feedback flow back to rule tuning? We designed three concrete feedback workflows: detection quality aggregation, noisy rule identification, and incident resolution capture.

**4. Vague on Elastic's actual mechanics.** The plan proposed "converting routing from workflow-first to agent-first orchestration" without explaining what that means in terms of Elastic's Agent Builder, the `kibana.post_agent_builder_converse` action type, or how workflows invoke agents. The architecture needs to be grounded in what the platform actually supports.

**5. Success criteria were unmeasurable.** ">80% of routine triage actions handled autonomously" — how do you define "routine"? How do you measure this? Without instrumentation and baseline metrics, these are aspirational statements, not success criteria.

The key takeaway: the Codex plan organised what was already known rather than pushing the design forward. It was a reasonable starting framework, but architecturally shallow.

---

## The Architecture We Built

### Six Layers, Not Four

The Codex plan proposed three planes (reasoning, execution, governance). We expanded this into a six-layer architecture that more precisely captures how the system actually works:

| Layer | Purpose |
|-------|---------|
| **1. Orchestrator** | Routes requests to specialist agents via semantic search on the agent registry |
| **2. Specialist Agents** | Domain experts with their own knowledge bases, tools, and system prompts |
| **3. Shared Tool Workflows** | Reusable YAML workflows (enrichment, search, cases, alerts, VirusTotal) |
| **4. Knowledge Management** | `kb-*` indices with semantic search for domain-specific knowledge |
| **5. Investigation Context** | Shared state between agents with evidence chains and action logs |
| **6. Governance Framework** | Risk-tiered controls checked before every impactful action |

Plus **feedback loops** that sit across all layers, continuously improving agent decision quality.

### Mesh vs. Hierarchy: We Chose Both

The original question was whether to use a hierarchical structure (single entry point) or a flat mesh (agents call each other freely). The answer turned out to be both, but for different access patterns:

- **User requests** go through the orchestrator, which uses semantic search on the agent registry to route to the right specialist. This prevents users from needing to know which agent handles what.
- **Agent-to-agent handoffs** happen directly. When the Analyst Agent needs threat intel enrichment, it calls the TI Agent directly — no routing through the orchestrator. The investigation context provides the shared state.
- **Alert-triggered workflows** bypass agents entirely for the initial triage, then hand off to the agent mesh once the alert is classified.

This gives you the discoverability of a hierarchical system with the flexibility of a mesh.

---

## Key Decision: Cases as Output, Not State

One of the most important architectural decisions was clarifying the role of Elastic Cases.

The initial instinct might be to use Cases as the coordination mechanism between agents — after all, cases can hold alerts, comments, and metadata. But cases are designed for human workflows: comments are free text, there's no structured schema for evidence confidence scores or pending governance approvals, and the API is optimised for CRUD operations, not high-frequency agent reads/writes.

Instead, we made a clean separation:

| Concern | Mechanism |
|---------|-----------|
| **Agent-to-agent state** | `investigation-contexts` index — structured nested data with evidence arrays, confidence scores, pending actions, and risk tiers |
| **Human-facing record** | Elastic Cases — created when an investigation warrants human visibility |

The flow works like this:

1. Agents investigate autonomously, sharing state through `investigation-contexts`.
2. When an investigation reaches a point that needs human eyes (confirmed attack, Tier 2 escalation), the agent creates a Case.
3. Cases accumulate context — agents attach alerts and add comments with analysis.
4. Humans review, tag true/false positives, and close with a resolution.
5. That resolution feeds back via the `record-incident-resolution` workflow into `kb-incidents`, improving future decisions.

Cases become a clean, readable output for humans rather than a cluttered dump of machine-to-machine chatter. The investigation context handles the structured coordination agents actually need.

---

## Key Decision: Embedding Model — E5 Over ELSER

The default embedding model for all semantic search in the mesh was originally ELSER (Elastic Learned Sparse EncodeR). We switched to `multilingual-e5-small` based on the nature of security data.

### Why ELSER wasn't the right fit

- **English-only optimisation.** ELSER is trained on English text and excels at natural language retrieval. But security data isn't all natural language — it contains IP addresses, file hashes, command-line arguments, registry paths, and structured technical indicators.
- **Sparse vectors for technical tokens.** ELSER produces sparse vectors that work by expanding and reweighting natural language tokens. A string like `powershell.exe -enc JABzAD0ATgBlAHcA` doesn't tokenise meaningfully in a vocabulary trained on English prose.

### Why E5 is better for this use case

- **Dense vectors capture semantic similarity** even for non-standard text. The model learns to map similar concepts close together in vector space regardless of surface-level token patterns.
- **Multilingual support.** Security teams deal with logs, scripts, and threat intel in many languages. E5 handles this natively.
- **Same 512-token constraint.** Both ELSER and E5 share the transformer architecture's token limit. This isn't an ELSER-specific issue — it's managed the same way in both cases through our `semantic_summary` field pattern, where we store a concise summary for embedding rather than attempting to embed entire documents.

The `semantic_text` field type in Elasticsearch abstracts away the embedding model — changing from ELSER to E5 required only updating the `inference_id` reference, not the index mapping structure. This is a strength of Elastic's design.

---

## Key Decision: Governance Tiers and Progressive Trust

The governance framework is based on a simple principle: **not all agent actions carry equal risk**. Rather than gating everything (which defeats the purpose of autonomy) or trusting everything (which is dangerous), we categorise by blast radius.

### The Three Tiers

**Tier 0 — Zero blast radius.** Searching indices, tagging alerts, enriching data, adding knowledge, creating notes. These actions cannot harm the environment. Gating them would slow the mesh to human speed and eliminate the value of automation. Agents execute these freely and log the decision.

**Tier 1 — Recoverable actions.** Creating a case, drafting a detection rule (created as disabled), running a contained playbook step. These have real impact but are reversible. They run autonomously *when* the agent demonstrates sufficient confidence and the scope stays within defined blast-radius limits. If the confidence is low or the scope exceeds the threshold, the action queues for review.

**Tier 2 — Potentially destructive.** Isolating a host from the network, enabling a rule in production, executing a command on an endpoint, applying policy changes. These could disrupt operations if wrong. They always require explicit human approval — the agent creates an Elastic Case for the analyst to review and approve directly in Kibana.

### How it works mechanically

Every action type has an entry in the `action-policies` index:

```json
{
  "action_type": "isolate_host",
  "risk_tier": "tier_2",
  "allowed_callers": ["security_analyst", "forensics"],
  "requires_approval": true,
  "approval_channel": "cases",
  "auto_approve": {
    "min_confidence": null,
    "max_blast_radius": null
  },
  "rollback_workflow": "unisolate-host.yaml",
  "ttl_minutes": 60
}
```

Before any action, the agent calls `check-action-policy` with the `action_type` and its own `agent_id`. The workflow returns the tier, whether approval is required, and any conditions. The agent then either proceeds, queues for auto-approval evaluation, or sends an approval request.

### Progressive trust

The tiers aren't static. The feedback loops measure agent accuracy over time: are detections producing false positives? Are investigations leading to correct classifications? As agents prove reliable, you expand Tier 1 thresholds (raise the blast-radius limit, lower the confidence requirement) or promote actions down a tier. The framework is designed so the mesh becomes *more* autonomous as it earns trust.

---

## Key Decision: Investigation Context as Shared State

The `investigation-contexts` index is arguably the most important architectural element — and the one the Codex plan missed entirely.

In a multi-agent system, the central question is: **how do agents share information about an ongoing investigation?** Without a shared state mechanism, each agent operates in isolation — the analyst doesn't know what the TI agent found, the forensics agent doesn't know what the analyst already triaged, and governance decisions have no evidence trail.

### What investigation context stores

```
investigation_id         — unique identifier
title                    — human-readable summary
trigger_type             — alert | manual | scheduled | agent_referral
status                   — open | investigating | escalated | resolved | closed
evidence[]               — nested array of findings from any agent
  ├── agent_id           — who contributed this
  ├── type               — ioc | log_entry | rule_match | enrichment | analysis
  ├── content            — the actual finding
  ├── confidence          — 0.0 to 1.0
  └── references[]       — supporting URLs or document IDs
actions_taken[]          — log of every action performed
pending_actions[]        — proposed actions awaiting governance approval
semantic_summary         — semantic_text field for finding similar investigations
```

### How it flows

1. An alert triggers the triage workflow, which creates a new investigation context.
2. The triage workflow assigns an analyst agent and records its initial findings as evidence.
3. The analyst enriches via the TI agent — those findings are added as more evidence entries.
4. The analyst proposes an action (e.g., isolate host) — added to `pending_actions` with risk tier and justification.
5. Governance checks the policy, requests approval if needed, and logs the decision.
6. On resolution, the feedback loop captures the outcome in `kb-incidents`.

Every agent that participates in the investigation writes to the same document. Every decision is auditable. Similar investigations can be found via semantic search on the summary field.

---

## Key Decision: Knowledge Base Strategy

Each agent has access to domain-specific knowledge bases — not one giant index, but purpose-built indices with the same mapping schema:

| Index | Purpose |
|-------|---------|
| `kb-detection-rules` | Rule metadata, logic, MITRE mappings, quality metrics |
| `kb-ecs-schema` | ECS field definitions for rule validation |
| `kb-mitre-attack` | ATT&CK techniques, sub-techniques, procedure examples |
| `kb-threat-intel` | Threat actor profiles, campaign tracking |
| `kb-ioc-history` | Historical IOC lookups and verdicts |
| `kb-incidents` | Resolved investigation outcomes — the learning corpus |
| `kb-playbooks` | Response procedures and runbooks |
| `kb-forensics` | Forensic analysis patterns and artifact references |
| `kb-compliance` | Regulatory frameworks, control mappings |
| `kb-soc-ops` | Rota schedules, escalation matrices, SLAs |
| `kb-runbooks` | Operational procedures |

All use the same mapping with a `semantic_summary` field backed by the E5 inference endpoint. Agents search using a single parameterised workflow (`semantic-knowledge-search.yaml`) — they just pass the `index_name` for whichever knowledge base they need.

Knowledge grows over time as agents write back what they learn:
- The Detection Engineering Agent writes rule quality metrics to `kb-detection-rules`.
- The feedback loops write false positive rates and noisy rule reports.
- The incident resolution workflow writes investigation outcomes to `kb-incidents`.

This is how the system becomes self-sustaining — each investigation makes future investigations better.

---

## Key Decision: Schema Normalisation

A core principle is that detections should target the common schema (ECS), not specific data sources. A rule written against `process.name` and `event.action` works regardless of whether the data comes from Elastic Defend, Sysmon, CrowdStrike, or any other integration that maps to ECS.

This matters because:
- **Portability** — rules work across data sources without rewriting.
- **Migration-friendly** — when migrating from Splunk or Sentinel, the detection logic is preserved but the query is rewritten to ECS fields and `logs-*`.
- **Coverage validation** — the Detection Engineering Agent can check field availability in the actual data using the `check-field-availability` workflow, so it only writes rules for fields that are actually being populated.

---

## Feedback Loops: Making It Self-Sustaining

The feedback loops are what transform this from "AI agents that do SOC tasks" into a genuinely self-improving system. Three scheduled workflows run continuously:

### 1. Detection Quality Feedback (daily)

Queries `.alerts-security*` for true positive and false positive tags, aggregates counts per detection rule, and writes a summary to `kb-detection-rules`. Over time, the Detection Engineering Agent can query its own knowledge base to see which rules are performing well and which need tuning.

### 2. Noisy Rule Identification (daily)

Identifies high-volume and high-false-positive rules by aggregating alert counts and FP tags. Rules that exceed thresholds are flagged in a report. The Detection Engineering Agent can then review and refine or disable them.

### 3. Incident Resolution Capture (on close)

When an investigation is closed, the resolution, outcome, and contributing factors are written to `kb-incidents`. Future investigations involving similar indicators or attack patterns can find these via semantic search, giving agents historical context they didn't have before.

---

## What's Not Automated (Yet)

Some things remain deliberately manual:

- **Agent creation in Agent Builder** — agent definitions live as YAML reference files in the repository, but the actual agents must be created through the Agent Builder UI. This is because Agent Builder manages conversation state, LLM integration, and tool binding in ways that aren't fully API-driven yet.
- **Knowledge base seeding** — the initial population of MITRE ATT&CK data, ECS schemas, and existing detection rules requires domain-specific ingestion that varies per deployment.
- **Web search integration** — the architecture supports plugging in any web search provider as an MCP tool or HTTP workflow. The reference implementation uses Brave Search as a template, but the production deployment will use a custom Vertex AI Cloud Run service.
- **Intune/Jamf policy application** — this is the end-state vision where agents can push configuration changes across the estate after an automated CAB process. The governance framework is designed to support this (it would be a Tier 2 action), but the actual integration workflows haven't been built yet.

---

## What We Learned

1. **State management is the hard problem.** Not routing, not tool calls, not prompt engineering. The challenge is designing how six autonomous agents share information safely and efficiently. Get this wrong and you have six smart agents that can't collaborate.

2. **Governance should be baked in from day one.** It's tempting to build the capabilities first and add controls later. But controls shape the architecture — the investigation context, the evidence chains, the decision logging — all exist partly because governance needs them.

3. **Cases and investigation context serve different audiences.** Trying to use one system for both agent coordination and human reporting leads to a design that does neither well.

4. **The embedding model matters for security data.** General-purpose NLP models optimised for English prose aren't ideal when your data includes IP addresses, base64-encoded commands, and registry paths. Dense vector models handle the diversity of security data better.

5. **Feedback loops are the killer feature.** Without them, you have automation. With them, you have a system that gets better at its job every day. This is the difference between "AI that does SOC tasks" and "an autonomous SOC."

6. **Elastic-native is a genuine advantage.** Most autonomous SOC projects fail because the AI layer is bolted onto a stack that can't execute actions. When your agents live inside the same platform that stores the data, manages the detections, handles the cases, and controls the endpoints, the action gap disappears.
